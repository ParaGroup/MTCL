/*
 * Master-worker microbenchmark using MTCL collectives (FANOUT + FANIN).
 *
 * Build:
 *   To compile this test MPI is mandatory, therefore you must compile with something like
 *     mpicxx -O3 -std=c++17 masterworkerMTCL.cpp -I<MTCL_INCLUDE_DIR> -L<MTCL_LIB_DIR> \
 *         -o masterworkerMTCL
 *   or, by using the Makefile: 
 *     make TPROTOCOL="TCP|UCX|MPI" masterworkerMTCL
 *
 * Run (TCP|UCX|... backends, MPI used only as launcher):
 *   mpirun -np <N> ./masterworkerMTCL <ntasks> <sleep_us> TCP [config.json]
 *
 * Run (MTCL MPI backend):
 *   mpirun -np <N> ./masterworkerMTCL <ntasks> <sleep_us> MPI  [config.json]
 *
 * Arguments:
 *   ntasks     Number of tasks generated by Master.
 *   sleep_us   Worker artificial delay per task (microseconds).
 *   protocol   "TCP" or "MPI" (default: TCP).
 *   config.json Optional MTCL config file. If omitted:
 *              - TCP: uses "test_masterworker.json".
 *              - MPI: generates a temporary config with hosts "0..N-1".
 *
 * Notes:
 * - Rank 0 is "Master". Ranks 1..N-1 are "Worker<rank>".
 * - The config file may contain more entries than N, only Master/Worker1..Worker(N-1)
 *   are actually used.
 */

#include <atomic>
#include <cerrno>
#include <chrono>
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <fstream>
#include <iostream>
#include <string>
#include <thread>
#include <unistd.h>
#include <vector>
#include <utility>

#include "mtcl.hpp"

#if defined(EXCLUDE_MPI) || !defined(ENABLE_MPI)
#include <iostream>
int main() {
    std::cerr << "This test requires MTCL built with MPI support\n";
	std::cerr << "Recompile using -DENABLE_MPI\n";
    return 1;
}
#else
#include "mpi.h"

using namespace MTCL;
using namespace std::chrono;

using task_t = int;
using result_t = int;

static size_t NTASKS = 1000;
static size_t SLEEP_US = 500;
static constexpr int DEFAULT_PORT = 42000;              // for TCP and UCX
static const std::string DEFAULT_LABEL{"listen_label"}; // for MQTT

static int read_int_env(const char* name, int defval = -1) {
    const char* v = std::getenv(name);
    if (!v) return defval;
    char* end = nullptr;
    long x = std::strtol(v, &end, 10);
    if (end == v || *end != '\0') return defval;
    if (x < 0 || x > 1000000) return defval;
    return static_cast<int>(x);
}
// get the rank and the total number of processes from "standard" env variables
static bool get_world_rank_size_from_env(int& rank, int& size) {
    // OpenMPI
    rank = read_int_env("OMPI_COMM_WORLD_RANK");
    size = read_int_env("OMPI_COMM_WORLD_SIZE");

    // MPICH/PMI
    if (rank < 0) rank = read_int_env("PMI_RANK");
    if (size < 0) size = read_int_env("PMI_SIZE");

    // Slurm
    if (rank < 0) rank = read_int_env("SLURM_PROCID");
    if (size < 0) size = read_int_env("SLURM_NTASKS");

    return (rank >= 0 && size > 0);
}

static int parse_int_arg(const char* s, int defval) {
    if (!s) return defval;
    char* end = nullptr;
    long v = std::strtol(s, &end, 10);
    if (end == s || *end != '\0') return defval;
    if (v <= 0 || v > 1000000000L) return defval;
    return static_cast<int>(v);
}

static std::string to_upper(std::string s) {
    for (char& c : s) {
        if (c >= 'a' && c <= 'z') c = char(c - 'a' + 'A');
    }
    return s;
}
// to automatically remove the temp config file (if used) 
struct TempFileGuard {
    std::string path;
    explicit TempFileGuard(std::string p) : path(std::move(p)) {}
    ~TempFileGuard() {
        if (!path.empty()) std::remove(path.c_str());
    }
    TempFileGuard(const TempFileGuard&) = delete;
    TempFileGuard& operator=(const TempFileGuard&) = delete;

    TempFileGuard(TempFileGuard&&) noexcept = default;
    TempFileGuard& operator=(TempFileGuard&&) noexcept = default;
};

static std::string write_temp_config(int world_size, int my_rank_for_unique_name,
									 const std::string& proto) {
    // For the MTCL MPI backend we set host to the MPI rank string ("0", "1", ...).
    const std::string path =
        "tmp_masterworker_r" + std::to_string(my_rank_for_unique_name) + "_" + std::to_string(getpid()) + ".json";

    std::ofstream os(path);
    if (!os) {
        std::cerr << "Cannot create temp config file: " << path << "\n";
        std::exit(1);
    }

    os << "{\n  \"components\" : [\n";

    os << "    {\n"
       << "      \"name\" : \"Master\",\n"
       << "      \"host\" : \"0\",\n";

	if (proto == "MPI") {
		os << "      \"protocols\" : [\"MPI\"],\n"
		   << "      \"listen-endpoints\" : [\"MPI:0\"]\n";
	} else if (proto == "UCX") {
		os << "      \"protocols\" : [\"UCX\"],\n"
		   << "      \"listen-endpoints\" : [\"UCX:localhost:" << std::to_string(DEFAULT_PORT) << "\"]\n";
	} else if (proto == "MQTT") {
		os << "      \"protocols\" : [\"MQTT\"],\n"
		   << "      \"listen-endpoints\" : [\"MQTT:" << DEFAULT_LABEL << "\"]\n";
	}
			
	os << "    }";

    for (int i = 1; i < world_size; i++) {
        os << ",\n    {\n"
           << "      \"name\" : \"Worker" << i << "\",\n"
           << "      \"host\" : \"" << i << "\",\n"
           << "      \"protocols\" : [\"" << proto << "\"]\n"
           << "    }";
    }

    os << "\n  ]\n}\n";
    return path;
}

static task_t generateTask() {
    static int counter = 0;
    return ++counter;
}

static result_t processTask(task_t& t) {
	if (SLEEP_US > 0) usleep((useconds_t)SLEEP_US);
    return t;
}

static bool recv_exact(HandleUser& h, void* buf, size_t n) {
    const ssize_t r = h.receive(buf, n);
    return r == (ssize_t)n;
}

static void processResults(HandleUser& fanin, std::atomic<bool>& ok_flag) {
    // Master receives exactly NTASKS results (one per task).
    for (size_t i = 0; i < NTASKS; i++) {
        result_t result{};
        if (!recv_exact(fanin, &result, sizeof(result_t))) {
			MTCL_ERROR("[Master]", "FANIN receive failed while collecting results at i=%ld, errno=%d (%s)\n", i, errno, std::strerror(errno));
            ok_flag.store(false);
            return;
        }
        // Process result if needed
        (void)result;
    }
}

int main(int argc, char** argv) {
    if (argc < 3) {
        std::cerr << "Usage: " << argv[0] << " <ntasks> <sleep_us> [TCP|MPI] [config.json]\n";
        return -1;
    }

    NTASKS = (size_t)parse_int_arg(argv[1], 1000);
    SLEEP_US = (size_t)parse_int_arg(argv[2], 500);

    std::string proto = (argc >= 4) ? to_upper(argv[3]) : std::string("TCP");
    std::string cfg = (argc >= 5) ? std::string(argv[4]) : std::string("");

    int env_rank = -1, env_size = -1;
    if (!get_world_rank_size_from_env(env_rank, env_size)) {
        std::cerr
            << "Cannot detect MPI rank/size from environment.\n"
            << "Run with mpirun, or ensure OMPI_COMM_WORLD_RANK/OMPI_COMM_WORLD_SIZE (or PMI/SLURM equivalents) are set.\n";
        return -1;
    }

    const std::string appName = (env_rank == 0) ? "Master" : ("Worker" + std::to_string(env_rank));

    TempFileGuard tmp_guard("");
    if (cfg.empty()) {
        if (proto == "TCP") {
            cfg = "test_masterworker.json";
        } else if (proto == "MPI" || proto == "UCX" || proto == "MQTT") {
            cfg = write_temp_config(env_size, env_rank, proto);
            tmp_guard = TempFileGuard(cfg);
        } else {
            std::cerr << "Unsupported protocol \"" << proto << "\". Use TCP, MPI, UCX, and MQTT.\n";
            return -1;
        }
    }

    if (Manager::init(appName, cfg) < 0) {
        std::cerr << "Manager::init failed for appName=" << appName
                  << ", errno=" << errno << " (" << std::strerror(errno) << ")\n";
        return -1;
    }

    // Now MPI must be initialized (either by MTCL or by the environment in your build).
    int rank = -1, numprocs = -1;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);

    if (rank != env_rank || numprocs != env_size) {
        std::cerr << "Mismatch between env rank/size and MPI rank/size (env "
                  << env_rank << "/" << env_size << ", mpi " << rank << "/" << numprocs << ")\n";
        Manager::finalize();
        return -1;
    }

    if (numprocs < 2) {
        std::cerr << "Need at least 2 processes.\n";
        Manager::finalize();
        return -1;
    }

    // Build team string "Master:Worker1:...:Worker(N-1)"
    std::string teamString{"Master:"};
    for (int i = 1; i < numprocs; i++) {
        teamString += "Worker" + std::to_string(i) + ":";
    }
    teamString.pop_back();

    // For TCP, UCX and MQTT, ensure the Master is listening
	if (rank == 0) {
		std::string ep{};
		if (proto == "TCP" || proto == "UCX") {
			ep = proto + ":localhost:" + std::to_string(DEFAULT_PORT);
		} else if (proto == "MQTT") {
			ep = "MQTT:" + DEFAULT_LABEL;
		}
		if (ep != "") { // ... for MPI we do not need to listen
			if (Manager::listen(ep) < 0) {
				std::cerr << "Manager::listen failed on " << ep
						  << ", errno=" << errno << " (" << std::strerror(errno) << ")\n";
				Manager::finalize();
				return -1;
			}
		}
	}

	// For MQTT, ensure all ranks start collective creation only after the master
	// has successfully registered the listening label on the broker.
	if (proto == "MQTT") {
		MPI_Barrier(MPI_COMM_WORLD);
	}
	
    // Create collectives
    HandleUser hg_fanout = Manager::createTeam(teamString, "Master", MTCL_FANOUT);
    HandleUser hg_fanin  = Manager::createTeam(teamString, "Master", MTCL_FANIN);

    if (!hg_fanout.isValid() || !hg_fanin.isValid()) {
        std::cerr << "createTeam failed (fanin/fanout invalid handle), errno=" << errno
                  << " (" << std::strerror(errno) << ")\n";
        Manager::finalize();
        return -1;
    }

    if (rank == 0) { // Master
        std::atomic<bool> ok{true};

        auto start = high_resolution_clock::now();
        std::thread collector(processResults, std::ref(hg_fanin), std::ref(ok));

        for (size_t i = 0; i < NTASKS; i++) {
            task_t task = generateTask();
            const ssize_t s = hg_fanout.send(&task, sizeof(task_t));
            if (s != (ssize_t)sizeof(task_t)) {
                std::cerr << "[Master] FANOUT send failed at i=" << i
                          << ", ret=" << s << ", errno=" << errno
                          << " (" << std::strerror(errno) << ")\n";
                ok.store(false);
                break;
            }
        }

        // Notify workers that no more tasks will arrive.
        hg_fanout.close();
        collector.join(); // joining the collector thread
        auto stop = high_resolution_clock::now();

        if (!ok.load()) {
            std::cerr << "[Master] Test failed.\n";
        } else {
            std::cout << "Time(ms): " << duration_cast<milliseconds>(stop - start).count() << "\n";
        }
        hg_fanin.close();
    } else {
        // Worker
        size_t processed = 0;
        while (true) {
            task_t task{};
            const ssize_t r = hg_fanout.receive(&task, sizeof(task_t));
            if (r <= 0) break;
            if (r != (ssize_t)sizeof(task_t)) {
				std::string name= "["+ appName +"]";
				MTCL_ERROR(name.c_str(), "FANOUT receive short read, ret=%ld, errno=%d (%s)\n",
						   r, errno, std::strerror(errno));
                break;
            }
			
            const result_t res = processTask(task);
            const ssize_t s = hg_fanin.send(&res, sizeof(result_t));
            if (s != (ssize_t)sizeof(result_t)) {
				std::string name= "["+ appName +"]";
				MTCL_ERROR(name.c_str(), "FANIN send failed, ret=%ld, errno=%d (%s)\n",
						   s, errno, std::strerror(errno));
                break;
            }
            processed++;
        }
        std::cout << appName << " processed " << processed << " tasks\n";

        hg_fanout.close();
        hg_fanin.close();
    }

    Manager::finalize();
    return 0;
}
#endif
